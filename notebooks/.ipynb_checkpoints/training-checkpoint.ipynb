{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb325da-6025-4011-934a-924b3efdb519",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e760a28-c544-4fff-8a10-622f9410cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d1a25c6-a16d-4833-8278-7c0a3eee59a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "# set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "078d726e-7a9c-437e-b722-e58b3788edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"civil_comments\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb6bef39-171e-4991-9da2-53e71ba0ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (5000, 8)\n",
      "column names: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n"
     ]
    }
   ],
   "source": [
    "# Examine the dataset and look at examples and columns\n",
    "import random\n",
    "random_idxs = random.sample(range(len(dataset)), 5)\n",
    "print(f\"dataset shape: {dataset.shape}\")\n",
    "print(f\"column names: {dataset.column_names}\")\n",
    "random_samples = dataset.select(random_idxs)\n",
    "# for sample in random_samples:\n",
    "#     print(sample['text'], \"\\n\", sample['toxicity'], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b801a-2d1b-49c4-81c6-072f2c80be45",
   "metadata": {},
   "source": [
    "```batched=True``` does the mapping in batches, hence is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29744ce1-2c02-4790-bf43-040b345f38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def preprocess(batch):\n",
    "    return {\n",
    "        \"text\": batch[\"text\"],\n",
    "        \"label\": [int(t > 0.5) for t in batch[\"toxicity\"]]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(\n",
    "    preprocess, \n",
    "    batched=True,\n",
    "    load_from_cache_file=True, \n",
    "    desc=\"Processing dataset\"\n",
    ")\n",
    "dataset = dataset.remove_columns([col for col in dataset.column_names if col not in [\"text\", \"label\"]])\n",
    "split = dataset.train_test_split(test_size=0.2, seed = 32)\n",
    "train_dataset = split[\"train\"]\n",
    "test_dataset = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "205f761c-43be-408e-837c-2af7a465b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bf350-84dc-45d9-8ced-6ce6414bc07b",
   "metadata": {},
   "source": [
    "padding is required when batching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7778669f-d2fa-4097-a5ae-522f87e86455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing test data: 100%|██████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 19533.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize in batches with truncation and padding\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing train data\"\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing test data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28739c95-afad-4afd-a4de-4c1fc199cff3",
   "metadata": {},
   "source": [
    "attention_mask tells the model itself during training or inference which parts of the input are real tokens (1) and which are padding (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b6ca79c-dc10-4371-9834-d93f777ef8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set format for PyTorch\n",
    "torch_columns = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "train_dataset.set_format(type=\"torch\", columns=torch_columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=torch_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186352b6-c4d4-4833-bf85-7436cf7fd14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (toxic-env)",
   "language": "python",
   "name": "toxic-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
